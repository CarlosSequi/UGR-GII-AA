funcionEjercicio2_2A = function(datos, label, iters, pesos){
resultados= ajusta_PLA(datos,label,iters,pesos)
resultados$iters
}
##Ejecutar el algoritmo 10 veces para el mismo vector de pesos
## produce los mismos resultados, por lo que basta con ejecutarlo
## una unica vez y ese será el valor medio
resultados=replicate(10,funcionEjercicio2_2A(datosEj2_2,labelEj2_2,1000,pesos))
pasosConverger_2A=round(mean(resultados))
print(sprintf("Numero de iteraciones medio para converger (pesos constantes): %d ",pasosConverger_2A))
## Variables utiles
pesos = c(0,0,1)
pasosConverger_2A=0
## Funcion para ejecutar el PLA devolviendo unicamente las iteraciones utilizadas
funcionEjercicio2_2A = function(datos, label, iters, pesos){
resultados= ajusta_PLA(datos,label,iters,pesos)
resultados$iters
}
##Ejecutar el algoritmo 10 veces para el mismo vector de pesos
## produce los mismos resultados, por lo que basta con ejecutarlo
## una unica vez y ese será el valor medio
resultados=replicate(10,funcionEjercicio2_2A(datosEj2_2,labelEj2_2,1000,pesos))
pasosConverger_2A=round(mean(resultados))
print(sprintf("Numero de iteraciones medio para converger (pesos constantes): %d ",pasosConverger_2A))
## Variables utiles
pesos = c(0,0,1)
pasosConverger_2A=0
## Funcion para ejecutar el PLA devolviendo unicamente las iteraciones utilizadas
funcionEjercicio2_2A = function(datos, label, iters, pesos){
resultados= ajusta_PLA(datos,label,iters,pesos)
resultados$iters
}
##Ejecutar el algoritmo 10 veces para el mismo vector de pesos
## produce los mismos resultados, por lo que basta con ejecutarlo
## una unica vez y ese será el valor medio
resultados=replicate(10,funcionEjercicio2_2A(datosEj2_2,labelEj2_2,1000,pesos))
pasosConverger_2A=round(mean(resultados))
print(sprintf("Numero de iteraciones medio para converger (pesos constantes): %d ",pasosConverger_2A))
## Funcion para ejecutar el PLA devolviendo unicamente las iteraciones utilizadas
## con un vector de pesos aleatorio
funcionEjercicio2_2B=function(datos, label){
pesos=runif(3,0,1)
resultados= ajusta_PLA(datos,label,1500,pesos)
resultados$iters
}
## Obtenemos la media de los pasos para converger
pasosConverger_2B=replicate(10,funcionEjercicio2_2B(datosEj2_2,labelEj2_2))
pasosConverger_2B=round(mean(pasosConverger_2B))
print(sprintf("Numero de iteraciones medio para converger (pesos aleatorios): %f ",pasosConverger_2B))
## Recuperamos los datos del ejercicio 2a
datosEj2_3 = ejercicio1_2
labelEj2_3 = etiquetasRuido
## Variables utiles
pesos = c(0,0,1)
pasosConverger_3A = 0
set.seed(3)
## Ejecutamos el algoritmo 10 veces y vemos cuanto tarda de media en
## converger, aunque es predecible lo que sucederá
resultados=replicate(10,funcionEjercicio2_2A(datosEj2_3,labelEj2_3,1500,pesos))
pasosConverger_3A = mean(resultados)
print(sprintf("Numero de iteraciones medio para converger (pesos constantes): %f ",pasosConverger_3A))
## Obtenemos la media de los pasos para converger en el caso B
pasosConverger_3B=replicate(10,funcionEjercicio2_2B(datosEj2_3,labelEj2_3))
pasosConverger_3B=mean(pasosConverger_3B)
print(sprintf("Numero de iteraciones medio para converger (pesos aleatorios): %f ",pasosConverger_3B))
## En una funcion que llamaremos leerDatosYEstructurar(fichero)
## englobaremos el funcionamiento de las lineas de codigo proporcionadas
## para utilizarla sobre distintos ficheros de datos
leerDatosYEstructurar=function(fichero){
# Lectura de fichero datos.train (2780 muestras con 256 valores en escala de gris)
# o datos.test
datos = read.table(fichero, quote="\"", comment.char="", stringsAsFactors=FALSE)
## Nos limitamos a seleccionar las instancias de los numeros
## 1 y 5 (aquellos en los que la columna 1 - V1 tengan un 1 o 5)
digitos15 = datos[datos$V1==1 | datos$V1==5,]
digitos = digitos15[,1]  # nombre de las etiquetas (si es 1 o 5)
ndigitos = nrow(digitos15) # numero de muestras de 1 y 5
## Se retira la clase y se monta una matriz 3D: 599*16*16
## Para cada instancia de 1 o 5, dimesionamos los 256 valores de
## escala de gris en una matriz 16x16
grises = array(unlist(subset(digitos15,select=-V1)),c(ndigitos,16,16))
## Eliminamos los dataframes que ya no vamos a utilizar
rm(datos)
rm(digitos15)
list(datos=grises,labels=digitos)
}
imagenesTrain=leerDatosYEstructurar("datos/zip.train")
## Para visualizar los 4 primeros
## ------------------------------------------------------------------------
par(mfrow=c(2,2))
for(i in 1:4){
imagen = imagenesTrain$datos[i,,16:1] # se rota para verlo bien
image(z=imagen)
}
## Etiquetas correspondientes a las 4 imágenes (los numeros que hemos dibujado)
imagenesTrain$labels[1:4]
## Para calcular el grado de simetria nos valemos de la funcion
## fsimetria(A) que calcula la diferencia en valor absoluto de la
## matriz A con la propia matriz A pero con las columnas invertidas
fsimetria = function(A){
A = abs(A-A[,ncol(A):1])
-mean(A)
}
## Calculamos el grado de simetria y el valor de intensidad media
gradoSimetriaTrain = apply(imagenesTrain$datos,1,fsimetria)
intensidadMediaTrain = apply(imagenesTrain$datos,1,mean)
par(mfrow=c(1,1))
## Pintamos las dos características
plot(intensidadMediaTrain,gradoSimetriaTrain,xlab = "Intensidad Promedio", ylab = "Simetria",col=imagenesTrain$labels+4)
## Recuperamos los datos del ejercicio 2a
datosEj2_2 = ejercicio1_2
labelEj2_2 = etiquetas
## Variables utiles
pesos = c(0,0,1)
pasosConverger_2A=0
## Funcion para ejecutar el PLA devolviendo unicamente las iteraciones utilizadas
funcionEjercicio2_2A = function(datos, label, iters, pesos){
resultados= ajusta_PLA(datos,label,iters,pesos)
resultados$iters
}
##Ejecutar el algoritmo 10 veces para el mismo vector de pesos
## produce los mismos resultados, por lo que basta con ejecutarlo
## una unica vez y ese será el valor medio
resultados=replicate(10,funcionEjercicio2_2A(datosEj2_2,labelEj2_2,500,pesos))
pasosConverger_2A=round(mean(resultados))
print(sprintf("Numero de iteraciones medio para converger (pesos constantes): %d ",
pasosConverger_2A))
## Funcion para ejecutar el PLA devolviendo unicamente las iteraciones utilizadas
## con un vector de pesos aleatorio
funcionEjercicio2_2B=function(datos, label){
pesos=runif(3,0,1)
resultados= ajusta_PLA(datos,label,500,pesos)
resultados$iters
}
## Obtenemos la media de los pasos para converger
pasosConverger_2B=replicate(10,funcionEjercicio2_2B(datosEj2_2,labelEj2_2))
pasosConverger_2B=round(mean(pasosConverger_2B))
print(sprintf("Numero de iteraciones medio para converger (pesos aleatorios): %f ",
pasosConverger_2B))
## Recuperamos los datos del ejercicio 2a
datosEj2_3 = ejercicio1_2
labelEj2_3 = etiquetasRuido
## Variables utiles
pesos = c(0,0,1)
pasosConverger_3A = 0
set.seed(3)
## Ejecutamos el algoritmo 10 veces y vemos cuanto tarda de media en
## converger, aunque es predecible lo que sucederá
resultados=replicate(10,funcionEjercicio2_2A(datosEj2_3,labelEj2_3,500,pesos))
pasosConverger_3A = mean(resultados)
print(sprintf("Numero de iteraciones medio para converger (pesos constantes): %f ",
pasosConverger_3A))
## Obtenemos la media de los pasos para converger en el caso B
pasosConverger_3B=replicate(10,funcionEjercicio2_2B(datosEj2_3,labelEj2_3))
pasosConverger_3B=mean(pasosConverger_3B)
print(sprintf("Numero de iteraciones medio para converger (pesos aleatorios): %f ",
pasosConverger_3B))
## Modificamos las etiquetas de tal forma que los 1s se queden con etiqueta
## 1 y los 5s cambiarán a -1.
etiquetasTrain=imagenesTrain$labels
etiquetasTrain[etiquetasTrain==5]=-1
## Creacion de la matriz de datos a partir de la intensidad promedio y el valor medio
datosTrain=matrix(c(intensidadMediaTrain,gradoSimetriaTrain),length(etiquetasTrain),2)
# Anidimos un 1 al final de nuestro vector de caracteristicas
datosTrain=cbind(datosTrain,1)
## Funcion Regress_Lin(datos, label) para ajustar un modelo de regresión
## lineal usando la transformación por SVD sobre la matriz de datosUnosCincos
## y pintar la solucion obtenida con los datos usados en el ajuste
## Vamos a utilizar la funcion svd(datos) para obtener la descomposición
## en valores singulares de la matriz de datos y obtener la matriz U y D
## para computar (X'X)^-1
Regress_Lin = function(datos,label){
# Calculamos la descomposicion en valores singurales
# y recuperamos sus valores
descompValSing = svd(datos)
D = descompValSing$d
V = descompValSing$v
#Calculamos la pseudoinversa de D
pInversaD = diag(1/D)
#Calculamos (X'X)^-1 = V*D^2*V'
traspuestaXInversa = V%*%(pInversaD^2) %*%t(V)
# Finalmente obtenemos la pseudoinversa
pseudoInversa = traspuestaXInversa%*%(t(datos))
# Nuestro vector de pesos final será PseudoInvX*label
W = (pseudoInversa%*%label)
}
## Ejecutamos el algoritmo de regresion lineal y comprobamos que funciona
regresionTrain = Regress_Lin(datosTrain,etiquetasTrain)
plot(datosTrain,col=etiquetasTrain+3)
coeficientesTrain=calcularCoeficientes(regresionTrain)
abline(coeficientesTrain[1],coeficientesTrain[2],col=3)
## Creamos una funcion para obtener el error en funcion del numero de muestras
## mal clasificadas que se llamara calculaErrorMalClasificadas(datos,label,pesos)
calcularErrorMalClasificadas = function(datos,label,pesos){
# Calculamos el producto de los datos por los pesos
malClasificadas = apply(datos,1,"%*%",pesos)
#Obtenemos cuantas etiquetas han sido mal clasificadas
numMalClasificadas = sum((sign(malClasificadas)!=label))
#Obtenemos el error
error = 100*numMalClasificadas/(length(label))
}
Ein = calcularErrorMalClasificadas(datosTrain,etiquetasTrain,regresionTrain)
printf(sprintf("El Ein es del %f porciento",Ein))
## Creamos una funcion para obtener el error en funcion del numero de muestras
## mal clasificadas que se llamara calculaErrorMalClasificadas(datos,label,pesos)
calcularErrorMalClasificadas = function(datos,label,pesos){
# Calculamos el producto de los datos por los pesos
malClasificadas = apply(datos,1,"%*%",pesos)
#Obtenemos cuantas etiquetas han sido mal clasificadas
numMalClasificadas = sum((sign(malClasificadas)!=label))
#Obtenemos el error
error = 100*numMalClasificadas/(length(label))
}
Ein = calcularErrorMalClasificadas(datosTrain,etiquetasTrain,regresionTrain)
print(sprintf("El Ein es del %f porciento",Ein))
## Para comparar la bondad del ajuste, utilizamos los datos de zip.test
imagenesTest=leerDatosYEstructurar("datos/zip.test")
## Calculamos el grado de simetria y el valor de intensidad media
gradoSimetriaTest = apply(imagenesTest$datos,1,fsimetria)
intensidadMediaTest = apply(imagenesTest$datos,1,mean)
## Modificamos las etiquetas de tal forma que los 1s se queden con etiqueta
## 1 y los 5s cambiarán a -1.
etiquetasTest=imagenesTest$labels
etiquetasTest[etiquetasTest==5]=-1
## Creacion de la matriz de datos a partir de la intensidad promedio y el valor medio
datosTest=matrix(c(intensidadMediaTest,gradoSimetriaTest),length(etiquetasTest),2)
# Anidimos un 1 al final de nuestro vector de caracteristicas
datosTest=cbind(datosTest,1)
## Ejecutamos el algoritmo de regresion lineal y comprobamos que funciona
regresionTest = Regress_Lin(datosTest,etiquetasTest)
plot(datosTest,col=etiquetasTest+3)
coeficientesTest=calcularCoeficientes(regresionTest)
abline(coeficientesTrain[1],coeficientesTrain[2],col=3)
## Calculamos Ein para luego comparar con el Eout de la muestra de Test
## donde Eout(W)=1/N||XW - y||^2
Eout = calcularErrorMalClasificadas(datosTest,etiquetasTest,regresionTrain)
## Calculamos la bondad del ajuste
bondadAjuste = 100 - (Eout-Ein)
print(sprintf("El Eout es del %f porciento",Eout))
print(sprintf("La bondad del ajuste es de un %f porciento",bondadAjuste))
## Apartado a)
## Generamos la muestra de entrenamiento de N=1000
datosExp1=simula_unif(1000,2,c(-1,1))
plot(datosExp1)
## Para comparar la bondad del ajuste, utilizamos los datos de zip.test
imagenesTest=leerDatosYEstructurar("datos/zip.test")
## Calculamos el grado de simetria y el valor de intensidad media
gradoSimetriaTest = apply(imagenesTest$datos,1,fsimetria)
intensidadMediaTest = apply(imagenesTest$datos,1,mean)
## Modificamos las etiquetas de tal forma que los 1s se queden con etiqueta
## 1 y los 5s cambiarán a -1.
etiquetasTest=imagenesTest$labels
etiquetasTest[etiquetasTest==5]=-1
## Creacion de la matriz de datos a partir de la intensidad promedio y el valor medio
datosTest=matrix(c(intensidadMediaTest,gradoSimetriaTest),length(etiquetasTest),2)
# Anidimos un 1 al final de nuestro vector de caracteristicas
datosTest=cbind(datosTest,1)
## Ejecutamos el algoritmo de regresion lineal y comprobamos que funciona
regresionTest = Regress_Lin(datosTest,etiquetasTest)
plot(datosTest,col=etiquetasTest+3)
coeficientesTest=calcularCoeficientes(regresionTest)
abline(coeficientesTrain[1],coeficientesTrain[2],col=3)
## Calculamos Ein para luego comparar con el Eout de la muestra de Test
## donde Eout(W)=1/N||XW - y||^2
Eout = calcularErrorMalClasificadas(datosTest,etiquetasTest,regresionTrain)
## Calculamos la bondad del ajuste
bondadAjuste = 100 - (Eout-Ein)
print(sprintf("El Eout es del %f porciento",Eout))
print(sprintf("La bondad del ajuste es de un %f porciento",bondadAjuste))
## Apartado b)
## Definimos la funcion que nos clasificará los puntos
f_Exp1=function(x1,x2){
sign((x1+0.2)^2 + x2^2 -0.6)
}
## Generamos las etiquetas con y sin ruido
etiquetasExp1 = f_Exp1(datosExp1[,1],datosExp1[,2])
etiquetasRuidoExp1 = asignarRuido(etiquetasExp1,10)
## Pintamos los puntos y comprobamos que funciona
par(mfrow=c(1,1))
plot(datosExp1,col=etiquetasRuidoExp1+2)
## Apartado c)
datosExp1_Aux=cbind(1,datosExp1)
pesosExp1=Regress_Lin(datosExp1_Aux,etiquetasRuidoExp1)
abline(-pesosExp1[1]/pesosExp1[2],-pesosExp1[2]/pesos[3])
## Apartado c)
datosExp1_Aux=cbind(1,datosExp1)
pesosExp1=Regress_Lin(datosExp1_Aux,etiquetasRuidoExp1)
plot(datosExp1)
abline(-pesosExp1[1]/pesosExp1[2],-pesosExp1[2]/pesos[3])
Ein_Exp1 = calcularErrorMalClasificadas(datosExp1_Aux,etiquetasRuidoExp1,pesosExp1)
print(sprintf("El error Ein es del %f porciento",Ein_Exp1))
## Apartado b)
## Definimos la funcion que nos clasificará los puntos
f_Exp1=function(x1,x2){
sign((x1+0.2)^2 + x2^2 -0.6)
}
## Generamos las etiquetas con y sin ruido
etiquetasExp1 = f_Exp1(datosExp1[,1],datosExp1[,2])
etiquetasRuidoExp1 = asignarRuido(etiquetasExp1,10)
## Pintamos los puntos y comprobamos que funciona
par(mfrow=c(1,1))
plot(datosExp1,col=etiquetasRuidoExp1+2)
## Apartado c)
datosExp1_Aux=cbind(1,datosExp1)
pesosExp1=Regress_Lin(datosExp1_Aux,etiquetasRuidoExp1)
plot(datosExp1)
abline(-pesosExp1[1]/pesosExp1[2],-pesosExp1[2]/pesos[3])
Ein_Exp1 = calcularErrorMalClasificadas(datosExp1_Aux,etiquetasRuidoExp1,pesosExp1)
print(sprintf("El error Ein es del %f porciento",Ein_Exp1))
## Apartado c)
datosExp1_Aux=cbind(1,datosExp1)
pesosExp1=Regress_Lin(datosExp1_Aux,etiquetasRuidoExp1)
plot(datosExp1, col=etiquetasRuidoExp1)
## Apartado c)
datosExp1_Aux=cbind(1,datosExp1)
pesosExp1=Regress_Lin(datosExp1_Aux,etiquetasRuidoExp1)
plot(datosExp1, col=etiquetasRuidoExp1+4)
abline(-pesosExp1[1]/pesosExp1[2],-pesosExp1[2]/pesos[3])
Ein_Exp1 = calcularErrorMalClasificadas(datosExp1_Aux,etiquetasRuidoExp1,pesosExp1)
print(sprintf("El error Ein es del %f porciento",Ein_Exp1))
## Apartado c)
datosExp1_Aux=cbind(1,datosExp1)
pesosExp1=Regress_Lin(datosExp1_Aux,etiquetasRuidoExp1)
plot(datosExp1, col=etiquetasRuidoExp1+2)
abline(-pesosExp1[1]/pesosExp1[2],-pesosExp1[2]/pesos[3])
Ein_Exp1 = calcularErrorMalClasificadas(datosExp1_Aux,etiquetasRuidoExp1,pesosExp1)
print(sprintf("El error Ein es del %f porciento",Ein_Exp1))
## Apartado d)
## Generamos 1000 muestras distintas (datos 2x1000)
funcionAuxiliar = function(datos){
sign((datos[,1]+0.2)^2 + datos[,2]^2 -0.6)
}
## Generamos las 1000 matrices
milMatrices = replicate(1000,simula_unif(1000,2,c(-1,1)))
## Generamos los 1000 conjuntos de etiquetas y les asignamos ruido
## (apply las genera por columnas, asi que la cambio por filas para mantener
##el mismo estandar en todos los ejercicios)
etiquetasMilMatrices = apply(milMatrices,3,funcionAuxiliar)
etiquetasMilMatrices = t(etiquetasMilMatrices)
etiquetasRuidoMilMatrices = apply(etiquetasMilMatrices,1,asignarRuido,porcentaje=10)
## Para cada matriz y cada conjunto de etiquetas, recuperamos los pesos
## lo hacemos una primera vez para no hacer el rbind con NULL
pesosMilMatrices = matrix(c(0,0,0),1,3,byrow = T)
Ein_MilMatrices = 0
## Funcion para calcular el Ein
calcularEinMilMatrices= function(milMatrices,pesosMilMatrices){
for(i in 1:dim(milMatrices)[3]){
# Aislamos la matriz y las etiquetas a utilizar
datos=cbind(milMatrices[,,i],1)
label = etiquetasRuidoMilMatrices[i,]
#Calculamos los pesos y el Ein
pesos = Regress_Lin(datos,label)
EinEach = calcularErrorMalClasificadas(datos,label,pesos)
#Guardamos los valores de pesos y Ein
pesosMilMatrices = rbind(pesosMilMatrices,t(pesos))
Ein_MilMatrices = cbind(Ein_MilMatrices,EinEach)
}
list(Ein=Ein_MilMatrices,Pesos=pesosMilMatrices)
}
## Borramos la primera fila de los pesos y la de los Ein
resultadosExp1 = calcularEinMilMatrices(milMatrices,pesosMilMatrices)
Ein_MilMatrices = resultadosExp1$Ein[1,2:dim(resultadosExp1$Ein)[2]]
Ein_MilMatrices = mean(Ein_MilMatrices)
print(sprintf("El error Ein para las mil matrices es del %f porciento",Ein_MilMatrices))
## Una vez tenemos los pesos de cada matriz, repetimos el experimento
## con nuevas muestras y etiquetas para calcular el Eout
milMatrices_Eout = replicate(1000,simula_unif(1000,2,c(-1,1)))
etiquetasMilMatrices_Eout = apply(milMatrices_Eout,3,funcionAuxiliar)
etiquetasMilMatrices_Eout = t(etiquetasMilMatrices_Eout)
etiquetasRuidoMilMatrices_Eout = apply(etiquetasMilMatrices_Eout,1,asignarRuido,porcentaje=10)
Eout_MilMatrices = 0
for(i in 1:dim(milMatrices_Eout)[3]){
# Aislamos la matriz y las etiquetas a utilizar
datos = cbind(milMatrices_Eout[,,i],1)
label = etiquetasRuidoMilMatrices_Eout[i,]
Eout_MilMatrices = cbind(Eout_MilMatrices,calcularErrorMalClasificadas(datos,label,resultadosExp1$Pesos[i,]))
}
## Eliminamos la primera componenente dado que es 0
Eout_MilMatrices = Eout_MilMatrices[1,2:dim(Eout_MilMatrices)[2]]
Eout_MilMatrices = mean(Eout_MilMatrices)
print(sprintf("El error Eout para las mil matrices de test es
del %f porciento",Eout_MilMatrices))
print(sprintf("La bondad del ajuste es de un %f porciento",100-abs(Eout_MilMatrices-Ein_MilMatrices)))
## Una vez tenemos los pesos de cada matriz, repetimos el experimento
## con nuevas muestras y etiquetas para calcular el Eout
milMatrices_Eout = replicate(1000,simula_unif(1000,2,c(-1,1)))
etiquetasMilMatrices_Eout = apply(milMatrices_Eout,3,funcionAuxiliar)
etiquetasMilMatrices_Eout = t(etiquetasMilMatrices_Eout)
etiquetasRuidoMilMatrices_Eout = apply(etiquetasMilMatrices_Eout,1,asignarRuido,porcentaje=10)
Eout_MilMatrices = 0
for(i in 1:dim(milMatrices_Eout)[3]){
# Aislamos la matriz y las etiquetas a utilizar
datos = cbind(milMatrices_Eout[,,i],1)
label = etiquetasRuidoMilMatrices_Eout[i,]
Eout_MilMatrices = cbind(Eout_MilMatrices,calcularErrorMalClasificadas(datos,label,resultadosExp1$Pesos[i,]))
}
## Eliminamos la primera componenente dado que es 0
Eout_MilMatrices = Eout_MilMatrices[1,2:dim(Eout_MilMatrices)[2]]
Eout_MilMatrices = mean(Eout_MilMatrices)
print(sprintf("El error Eout para las mil matrices de test es
del %f porciento",Eout_MilMatrices))
print(sprintf("La bondad del ajuste es de un %f porciento",100-abs(Eout_MilMatrices-Ein_MilMatrices)))
## Apartado d)
## Generamos 1000 muestras distintas (datos 2x1000)
funcionAuxiliar = function(datos){
sign((datos[,1]+0.2)^2 + datos[,2]^2 -0.6)
}
## Generamos las 1000 matrices
milMatrices = replicate(1000,simula_unif(1000,2,c(-1,1)))
## Generamos los 1000 conjuntos de etiquetas y les asignamos ruido
## (apply las genera por columnas, asi que la cambio por filas para mantener
##el mismo estandar en todos los ejercicios)
etiquetasMilMatrices = apply(milMatrices,3,funcionAuxiliar)
etiquetasMilMatrices = t(etiquetasMilMatrices)
etiquetasRuidoMilMatrices = apply(etiquetasMilMatrices,1,asignarRuido,porcentaje=10)
## Para cada matriz y cada conjunto de etiquetas, recuperamos los pesos
## lo hacemos una primera vez para no hacer el rbind con NULL
pesosMilMatrices = matrix(c(0,0,0),1,3,byrow = T)
Ein_MilMatrices = 0
## Funcion para calcular el Ein
calcularEinMilMatrices= function(milMatrices,pesosMilMatrices){
for(i in 1:dim(milMatrices)[3]){
# Aislamos la matriz y las etiquetas a utilizar
datos=cbind(milMatrices[,,i],1)
label = etiquetasRuidoMilMatrices[i,]
#Calculamos los pesos y el Ein
pesos = Regress_Lin(datos,label)
EinEach = calcularErrorMalClasificadas(datos,label,pesos)
#Guardamos los valores de pesos y Ein
pesosMilMatrices = rbind(pesosMilMatrices,t(pesos))
Ein_MilMatrices = cbind(Ein_MilMatrices,EinEach)
}
list(Ein=Ein_MilMatrices,Pesos=pesosMilMatrices)
}
## Borramos la primera fila de los pesos y la de los Ein
resultadosExp1 = calcularEinMilMatrices(milMatrices,pesosMilMatrices)
Ein_MilMatrices = resultadosExp1$Ein[1,2:dim(resultadosExp1$Ein)[2]]
Ein_MilMatrices = mean(Ein_MilMatrices)
print(sprintf("El error Ein para las mil matrices de entrenamiento es del %f porciento",
Ein_MilMatrices))
## Una vez tenemos los pesos de cada matriz, repetimos el experimento
## con nuevas muestras y etiquetas para calcular el Eout
milMatrices_Eout = replicate(1000,simula_unif(1000,2,c(-1,1)))
etiquetasMilMatrices_Eout = apply(milMatrices_Eout,3,funcionAuxiliar)
etiquetasMilMatrices_Eout = t(etiquetasMilMatrices_Eout)
etiquetasRuidoMilMatrices_Eout = apply(etiquetasMilMatrices_Eout,1,asignarRuido,porcentaje=10)
Eout_MilMatrices = 0
for(i in 1:dim(milMatrices_Eout)[3]){
# Aislamos la matriz y las etiquetas a utilizar
datos = cbind(milMatrices_Eout[,,i],1)
label = etiquetasRuidoMilMatrices_Eout[i,]
Eout_MilMatrices = cbind(Eout_MilMatrices,calcularErrorMalClasificadas(datos,label,resultadosExp1$Pesos[i,]))
}
## Eliminamos la primera componenente dado que es 0
Eout_MilMatrices = Eout_MilMatrices[1,2:dim(Eout_MilMatrices)[2]]
Eout_MilMatrices = mean(Eout_MilMatrices)
print(sprintf("El error Eout para las mil matrices de test es del %f porciento",
Eout_MilMatrices))
print(sprintf("La bondad del ajuste es de un %f porciento",100-abs(Eout_MilMatrices-Ein_MilMatrices)))
## Apartado a)
## Repetimos el mismo experimento anterior pero ahora con el vector de caracte-
## risticas fi 2(x) = (1, x1, x2, x1x2, (x1)^2, (x2)^2)
## Generamos la muestra de entrenamiento de N=1000
datosExp2=simula_unif(1000,2,c(-1,1))
plot(datosExp2)
## Definimos la funcion que nos clasificará los puntos en el Experimento 2
f_Exp2=function(x1,x2){
sign((x1+0.2)^2 + x2^2 -0.6)
}
## Generamos las etiquetas con y sin ruido
etiquetasExp2 = f_Exp2(datosExp2[,1],datosExp2[,2])
etiquetasRuidoExp2 = asignarRuido(etiquetasExp2,10)
## Generamos el vector de caracteristicas completo
datosExp2 = cbind(datosExp2, datosExp2[,1]*datosExp2[,2])
datosExp2 = cbind(datosExp2,datosExp2[,1]^2)
datosExp2 = cbind(datosExp2,datosExp2[,2]^2)
datosExp2_Aux = cbind(1,datosExp2)
## Calculamos el Ein del experimento 2 (primera parte)
pesosExp2=Regress_Lin(datosExp2_Aux,etiquetasRuidoExp2)
Ein_Exp2 = calcularErrorMalClasificadas(datosExp2_Aux,etiquetasRuidoExp2,pesosExp2)
print(sprintf("El error Ein es del %f porciento",Ein_Exp2))
EinTotales_Exp2 = 0
EoutTotales_Exp2 = 0
for(i in 1:1000){
#Calculamos las matrices y etiquetas de entrenamiento (con ruido)
matrizTrain_Exp2=simula_unif(1000,2,c(-1,1))
etiquetasTrain_Exp2 = f_Exp2(matrizTrain_Exp2[,1],matrizTrain_Exp2[,2])
etiquetasTrain_Exp2 = asignarRuido(etiquetasTrain_Exp2,10)
matrizTrain_Exp2=cbind(1,matrizTrain_Exp2,matrizTrain_Exp2[,1]*matrizTrain_Exp2[,2],
matrizTrain_Exp2[,1]^2,matrizTrain_Exp2[,2]^2)
#Calculamos los pesos de entrenamiento y el Ein de entrenamiento
pesosTrain = Regress_Lin(matrizTrain_Exp2,etiquetasTrain_Exp2)
EinTrain_Exp2 = calcularErrorMalClasificadas(matrizTrain_Exp2,etiquetasTrain_Exp2,pesosTrain)
EinTotales_Exp2 = cbind(EinTotales_Exp2,EinTrain_Exp2)
#Calculamos las matrices y etiquetas de Train (con ruido)
matrizTest_Exp2=simula_unif(1000,2,c(-1,1))
etiquetasTest_Exp2 = f_Exp2(matrizTest_Exp2[,1],matrizTest_Exp2[,2])
etiquetasTest_Exp2 = asignarRuido(etiquetasTest_Exp2,10)
matrizTest_Exp2=cbind(1,matrizTest_Exp2,matrizTest_Exp2[,1]*matrizTest_Exp2[,2],
matrizTest_Exp2[,1]^2,matrizTest_Exp2[,2]^2)
#Calculamos el Eout
EoutTest_Exp2 = calcularErrorMalClasificadas(matrizTest_Exp2,etiquetasTest_Exp2,pesosTrain)
EoutTotales_Exp2 = cbind(EoutTotales_Exp2,EoutTest_Exp2)
}
## Calculamos Ein de las mil matrices
EinTotales_Exp2 = EinTotales_Exp2[1,2:dim(EinTotales_Exp2)[2]]
Ein_MilMatrices_Exp2 = mean(EinTotales_Exp2)
## Calculamos Eout de las mil matrices
EoutTotales_Exp2 = EoutTotales_Exp2[1,2:dim(EoutTotales_Exp2)[2]]
Eout_MilMatrices_Exp2 = mean(EoutTotales_Exp2)
print(sprintf("El Ein del experimento 2 es del %f porciento",Ein_MilMatrices_Exp2))
print(sprintf("El Eout del experimento 2 es del %f porciento",Eout_MilMatrices_Exp2))
